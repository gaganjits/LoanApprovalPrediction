{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project aims to build an automated loan approval prediction system using machine learning. The system analyzes various applicant features to predict whether a loan application should be approved or rejected.\n",
    "\n",
    "### Objectives:\n",
    "- Increase efficiency in loan processing\n",
    "- Improve accuracy of loan decisions\n",
    "- Enhance fairness and reduce bias\n",
    "- Enable scalability for high-volume applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Imbalanced data handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('loan_approval_dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display first few rows\n",
    "print(\"\\n=== First 5 rows of the dataset ===\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset info\n",
    "print(\"\\n=== Dataset Information ===\")\n",
    "df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Statistical summary\n",
    "print(\"\\n=== Statistical Summary ===\")\n",
    "df.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify column types\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check target variable distribution\n",
    "if 'loan_status' in df.columns:\n",
    "    target_col = 'loan_status'\n",
    "elif 'Loan_Status' in df.columns:\n",
    "    target_col = 'Loan_Status'\n",
    "else:\n",
    "    # Find likely target column\n",
    "    target_col = [col for col in df.columns if 'status' in col.lower() or 'approved' in col.lower()]\n",
    "    target_col = target_col[0] if target_col else df.columns[-1]\n",
    "\n",
    "print(f\"\\n=== Target Variable: {target_col} ===\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df[target_col].value_counts(normalize=True) * 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df[target_col].value_counts().plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title(f'Distribution of {target_col}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(target_col)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "df[target_col].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                     colors=['#FF6B6B', '#4ECDC4'], startangle=90)\n",
    "axes[1].set_title(f'Proportion of {target_col}', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of numerical features\n",
    "numerical_features = [col for col in numerical_cols if col != target_col]\n",
    "\n",
    "if len(numerical_features) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(numerical_features):\n",
    "        if idx < len(axes):\n",
    "            df[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "            axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(numerical_features), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of categorical features\n",
    "categorical_features = [col for col in categorical_cols if col != target_col]\n",
    "\n",
    "if len(categorical_features) > 0:\n",
    "    n_cols = 2\n",
    "    n_rows = (len(categorical_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(categorical_features):\n",
    "        if idx < len(axes):\n",
    "            df[col].value_counts().plot(kind='bar', ax=axes[idx], color='skyblue', edgecolor='black')\n",
    "            axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "            axes[idx].set_xlabel(col)\n",
    "            axes[idx].set_ylabel('Count')\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(categorical_features), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Correlation matrix for numerical features\n",
    "if len(numerical_features) > 1:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlation_matrix = df[numerical_features].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of Numerical Features', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Box plots to detect outliers\n",
    "if len(numerical_features) > 0:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(numerical_features):\n",
    "        if idx < len(axes):\n",
    "            df.boxplot(column=col, ax=axes[idx])\n",
    "            axes[idx].set_title(f'Box Plot of {col}', fontweight='bold')\n",
    "            axes[idx].set_ylabel(col)\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(numerical_features), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"Starting data preprocessing...\")\n",
    "print(f\"Initial shape: {df_processed.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle missing values\n",
    "print(\"\\n=== Handling Missing Values ===\")\n",
    "\n",
    "# For numerical columns: fill with median\n",
    "for col in numerical_features:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        median_value = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled {col} missing values with median: {median_value}\")\n",
    "\n",
    "# For categorical columns: fill with mode\n",
    "for col in categorical_features:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        mode_value = df_processed[col].mode()[0]\n",
    "        df_processed[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled {col} missing values with mode: {mode_value}\")\n",
    "\n",
    "print(f\"\\nMissing values after imputation: {df_processed.isnull().sum().sum()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Remove duplicates if any\n",
    "before_dup = len(df_processed)\n",
    "df_processed.drop_duplicates(inplace=True)\n",
    "after_dup = len(df_processed)\n",
    "print(f\"\\nRemoved {before_dup - after_dup} duplicate rows\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Encode categorical variables\n",
    "print(\"\\n=== Encoding Categorical Variables ===\")\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col}: {list(le.classes_)}\")\n",
    "\n",
    "# Encode target variable if it's categorical\n",
    "if df_processed[target_col].dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    df_processed[target_col] = le_target.fit_transform(df_processed[target_col])\n",
    "    label_encoders[target_col] = le_target\n",
    "    print(f\"\\nEncoded target '{target_col}': {list(le_target.classes_)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle outliers using IQR method (optional - can be commented out if needed)\n",
    "print(\"\\n=== Handling Outliers (IQR Method) ===\")\n",
    "\n",
    "def remove_outliers_iqr(df, columns, factor=1.5):\n",
    "    df_clean = df.copy()\n",
    "    outliers_removed = 0\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "        after = len(df_clean)\n",
    "        removed = before - after\n",
    "        \n",
    "        if removed > 0:\n",
    "            outliers_removed += removed\n",
    "            print(f\"{col}: Removed {removed} outliers (bounds: [{lower_bound:.2f}, {upper_bound:.2f}])\")\n",
    "    \n",
    "    return df_clean, outliers_removed\n",
    "\n",
    "# Uncomment the next line to remove outliers\n",
    "# df_processed, total_outliers = remove_outliers_iqr(df_processed, numerical_features)\n",
    "# print(f\"\\nTotal outliers removed: {total_outliers}\")\n",
    "\n",
    "print(f\"Shape after preprocessing: {df_processed.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature engineering - create new features if applicable\n",
    "print(\"=== Feature Engineering ===\")\n",
    "\n",
    "# Example: Create debt-to-income ratio if income and loan amount columns exist\n",
    "income_cols = [col for col in df_processed.columns if 'income' in col.lower()]\n",
    "loan_cols = [col for col in df_processed.columns if 'loan' in col.lower() and 'amount' in col.lower()]\n",
    "\n",
    "if income_cols and loan_cols:\n",
    "    income_col = income_cols[0]\n",
    "    loan_col = loan_cols[0]\n",
    "    df_processed['debt_to_income_ratio'] = df_processed[loan_col] / (df_processed[income_col] + 1)\n",
    "    print(f\"Created feature: debt_to_income_ratio using {loan_col} and {income_col}\")\n",
    "\n",
    "# Example: Create age groups if age column exists\n",
    "age_cols = [col for col in df_processed.columns if 'age' in col.lower()]\n",
    "if age_cols:\n",
    "    age_col = age_cols[0]\n",
    "    df_processed['age_group'] = pd.cut(df_processed[age_col], \n",
    "                                        bins=[0, 25, 35, 50, 100],\n",
    "                                        labels=[0, 1, 2, 3])\n",
    "    df_processed['age_group'] = df_processed['age_group'].astype(int)\n",
    "    print(f\"Created feature: age_group from {age_col}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_processed.shape}\")\n",
    "print(f\"Feature columns: {df_processed.columns.tolist()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop(columns=[target_col])\n",
    "y = df_processed[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTesting target distribution:\\n{y_test.value_counts()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle class imbalance using SMOTE\n",
    "print(\"\\n=== Handling Class Imbalance ===\")\n",
    "print(f\"Before SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"After SMOTE: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
    "print(f\"Training set shape after SMOTE: {X_train_balanced.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_estimators=100, eval_metric='logloss'),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(\"Initialized models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "print(\"\\n=== Training Models ===\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate ROC AUC if probability predictions available\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        except:\n",
    "            roc_auc = None\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1-Score: {f1:.4f} | CV: {cv_mean:.4f} (+/- {cv_std:.4f})\\n\")\n",
    "\n",
    "print(\"Model training completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n=== Model Performance Comparison ===\")\n",
    "print(results_df.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "results_df.plot(x='Model', y='Accuracy', kind='bar', ax=axes[0, 0], color='skyblue', legend=False)\n",
    "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Model')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "# Precision comparison\n",
    "results_df.plot(x='Model', y='Precision', kind='bar', ax=axes[0, 1], color='lightgreen', legend=False)\n",
    "axes[0, 1].set_title('Model Precision Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Model')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# Recall comparison\n",
    "results_df.plot(x='Model', y='Recall', kind='bar', ax=axes[1, 0], color='lightcoral', legend=False)\n",
    "axes[1, 0].set_title('Model Recall Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Model')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# F1-Score comparison\n",
    "results_df.plot(x='Model', y='F1-Score', kind='bar', ax=axes[1, 1], color='plum', legend=False)\n",
    "axes[1, 1].set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_ylabel('F1-Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Model comparison chart saved as 'model_comparison.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Predictions with best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n=== Classification Report for {best_model_name} ===\")\n",
    "print(classification_report(y_test, y_pred_best))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, square=True,\n",
    "            xticklabels=['Rejected', 'Approved'],\n",
    "            yticklabels=['Rejected', 'Approved'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROC Curve\n",
    "if y_pred_proba_best is not None:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'ROC Curve - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(\"ROC curve saved as 'roc_curve.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature Importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== Feature Importance - {best_model_name} ===\")\n",
    "    print(feature_importance.to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(feature_importance['Feature'][:15], feature_importance['Importance'][:15], color='teal')\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature importance chart saved as 'feature_importance.png'\")\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': best_model.coef_[0]\n",
    "    }).sort_values('Coefficient', ascending=False, key=abs)\n",
    "    \n",
    "    print(f\"\\n=== Feature Coefficients - {best_model_name} ===\")\n",
    "    print(feature_importance.to_string(index=False))\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient'][:15]]\n",
    "    plt.barh(feature_importance['Feature'][:15], feature_importance['Coefficient'][:15], color=colors)\n",
    "    plt.xlabel('Coefficient', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title(f'Top 15 Feature Coefficients - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature coefficients chart saved as 'feature_coefficients.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for best model\n",
    "print(f\"\\n=== Hyperparameter Tuning for {best_model_name} ===\")\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"Performing GridSearchCV for {best_model_name}...\")\n",
    "    print(\"This may take several minutes...\\n\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=best_model,\n",
    "        param_grid=param_grids[best_model_name],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use tuned model for final predictions\n",
    "    best_model_tuned = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model_tuned.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    f1_tuned = f1_score(y_test, y_pred_tuned, average='weighted')\n",
    "    \n",
    "    print(f\"\\nTuned model performance:\")\n",
    "    print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(f\"F1-Score: {f1_tuned:.4f}\")\n",
    "    \n",
    "    print(f\"\\nClassification Report (Tuned Model):\")\n",
    "    print(classification_report(y_test, y_pred_tuned))\n",
    "    \n",
    "    best_model = best_model_tuned\n",
    "else:\n",
    "    print(f\"No predefined parameter grid for {best_model_name}\")\n",
    "    print(\"Using default parameters...\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the best model and preprocessing artifacts\n",
    "import pickle\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "with open('models/best_loan_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"Best model ({best_model_name}) saved to 'models/best_loan_model.pkl'\")\n",
    "\n",
    "# Save scaler\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved to 'models/scaler.pkl'\")\n",
    "\n",
    "# Save label encoders\n",
    "with open('models/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(\"Label encoders saved to 'models/label_encoders.pkl'\")\n",
    "\n",
    "# Save feature names\n",
    "with open('models/feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(X.columns.tolist(), f)\n",
    "print(\"Feature names saved to 'models/feature_names.pkl'\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': accuracy_score(y_test, best_model.predict(X_test_scaled)),\n",
    "    'f1_score': f1_score(y_test, best_model.predict(X_test_scaled), average='weighted'),\n",
    "    'features': X.columns.tolist(),\n",
    "    'target_column': target_col,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('models/model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"Model metadata saved to 'models/model_metadata.pkl'\")\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Make predictions on new data\n",
    "def predict_loan_approval(new_data_dict):\n",
    "    \"\"\"\n",
    "    Predict loan approval for new applicant data.\n",
    "    \n",
    "    Parameters:\n",
    "    new_data_dict: dict with feature names as keys and values\n",
    "    \n",
    "    Returns:\n",
    "    prediction: 0 (Rejected) or 1 (Approved)\n",
    "    probability: Probability of approval\n",
    "    \"\"\"\n",
    "    # Load artifacts\n",
    "    with open('models/best_loan_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open('models/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    with open('models/label_encoders.pkl', 'rb') as f:\n",
    "        encoders = pickle.load(f)\n",
    "    with open('models/feature_names.pkl', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    \n",
    "    # Create dataframe\n",
    "    new_data = pd.DataFrame([new_data_dict])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for col, encoder in encoders.items():\n",
    "        if col in new_data.columns and col != target_col:\n",
    "            try:\n",
    "                new_data[col] = encoder.transform(new_data[col])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for feature in features:\n",
    "        if feature not in new_data.columns:\n",
    "            new_data[feature] = 0\n",
    "    \n",
    "    new_data = new_data[features]\n",
    "    \n",
    "    # Scale features\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(new_data_scaled)[0]\n",
    "    probability = model.predict_proba(new_data_scaled)[0][1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test with a sample from test set\n",
    "sample_idx = 0\n",
    "sample_data = X_test.iloc[sample_idx].to_dict()\n",
    "\n",
    "pred, prob = predict_loan_approval(sample_data)\n",
    "\n",
    "print(\"\\n=== Sample Prediction ===\")\n",
    "print(f\"Input features: {sample_data}\")\n",
    "print(f\"\\nPrediction: {'Approved' if pred == 1 else 'Rejected'}\")\n",
    "if prob is not None:\n",
    "    print(f\"Approval Probability: {prob:.2%}\")\n",
    "print(f\"Actual: {'Approved' if y_test.iloc[sample_idx] == 1 else 'Rejected'}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOAN APPROVAL PREDICTION SYSTEM - PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATASET OVERVIEW\")\n",
    "print(f\"   - Total records: {len(df)}\")\n",
    "print(f\"   - Number of features: {df.shape[1] - 1}\")\n",
    "print(f\"   - Target variable: {target_col}\")\n",
    "\n",
    "print(f\"\\n2. BEST MODEL\")\n",
    "print(f\"   - Algorithm: {best_model_name}\")\n",
    "print(f\"   - Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"   - Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
    "print(f\"   - Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
    "print(f\"   - F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "if results_df.iloc[0]['ROC AUC'] is not None:\n",
    "    print(f\"   - ROC AUC: {results_df.iloc[0]['ROC AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. KEY ACHIEVEMENTS\")\n",
    "print(f\"   - Automated loan decision process\")\n",
    "print(f\"   - Handled class imbalance using SMOTE\")\n",
    "print(f\"   - Evaluated {len(models)} different algorithms\")\n",
    "print(f\"   - Created comprehensive visualization and reporting\")\n",
    "\n",
    "print(f\"\\n4. DELIVERABLES\")\n",
    "print(f\"   - Trained model saved in 'models/' directory\")\n",
    "print(f\"   - Preprocessing artifacts (scaler, encoders)\")\n",
    "print(f\"   - Model performance visualizations\")\n",
    "print(f\"   - Prediction pipeline ready for deployment\")\n",
    "\n",
    "print(f\"\\n5. BUSINESS IMPACT\")\n",
    "print(f\"   - Faster loan processing time\")\n",
    "print(f\"   - Consistent and objective decision-making\")\n",
    "print(f\"   - Reduced manual workload\")\n",
    "print(f\"   - Improved customer satisfaction\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
